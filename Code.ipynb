{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Importing NumPy \n",
    "import pandas as pd  # Importing Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load .csv dataset into a pandas dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"https://drive.google.com/uc?export=download&id=1XyhVIZaKYZczlM2alun_fofilqTBq_9c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shows the top 5 records of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuple representing the number of rows and columns in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the DataFrame, including data types and non-null counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing the statistical summary of numerical type data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the unique values in the 'smoking_status' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.smoking_status.unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical summary of categorical type data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the count of missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the missing values percentage for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_percentage = (df.isnull().mean() * 100).round(2)\n",
    "\n",
    "# Display the missing values percentage for each column\n",
    "print(\"Missing Values Percentage:\\n\")\n",
    "print(missing_values_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # df['bmi'] = df['bmi'].fillna(df['bmi'].median())\n",
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Matplotlib and Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age Distribution with Stroke Incidence-Histplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "custom_palette = [\"#457b9d\", \"#e63946\"]  \n",
    "sns.histplot(data=df, x='age', hue='stroke', multiple='stack', kde=True, bins=30, palette=custom_palette)  \n",
    "plt.title('Age Distribution with Stroke Incidence')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertension and Heart Disease vs Stroke Incidence-Countplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "custom_palette = [\"#457b9d\", \"#ef233c\"]\n",
    "\n",
    "# First subplot for Hypertension vs Stroke Incidence\n",
    "sns.countplot(data=df, x='hypertension', hue='stroke', palette=custom_palette, ax=axes[0])\n",
    "axes[0].set_title('Hypertension vs Stroke Incidence')\n",
    "axes[0].set_xlabel('Hypertension (1 = Yes, 0 = No)')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Second subplot for Heart Disease vs Stroke Incidence\n",
    "sns.countplot(data=df, x='heart_disease', hue='stroke', palette=custom_palette, ax=axes[1])\n",
    "axes[1].set_title('Heart Disease vs Stroke Incidence')\n",
    "axes[1].set_xlabel('Heart Disease (1 = Yes, 0 = No)')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.4)  # Adjust wspace to control the space between plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Glucose Level Distribution by Stroke-Kdeplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "custom_palette = [\"#457b9d\", \"#ef233c\"]\n",
    "sns.kdeplot(data=df, x='avg_glucose_level', hue='stroke', fill=True, palette=custom_palette)\n",
    "plt.title('Average Glucose Level Distribution by Stroke')\n",
    "plt.xlabel('Average Glucose Level')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BMI Distribution with Stroke Incidence-Histplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "custom_palette = [\"#457b9d\", \"#ef233c\"]\n",
    "sns.histplot(data=df, x='bmi', hue='stroke', kde=True, bins=30, palette=custom_palette)\n",
    "plt.title('BMI Distribution with Stroke Incidence')\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Distribution by Stroke Incidence-Countplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "custom_palette = [\"#457b9d\", \"#ef233c\"]\n",
    "sns.countplot(data=df, x='gender', hue='stroke', palette=custom_palette)\n",
    "plt.title(\"Gender Distribution by Stroke Incidence\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age Distribution by Smoking Status and Stroke Incidence-Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "custom_palette = [\"#457b9d\", \"#ef233c\"]\n",
    "sns.boxplot(data=df, x=\"smoking_status\", y=\"age\", hue=\"stroke\", palette=custom_palette)\n",
    "\n",
    "plt.title(\"Age Distribution by Smoking Status and Stroke Incidence\")\n",
    "plt.xlabel(\"Smoking Status\")\n",
    "plt.ylabel(\"Age\")\n",
    "plt.legend(title='Stroke Incidence', labels=['No Stroke (0)', 'Stroke (1)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter Plot of Age vs BMI by Stroke Incidence-Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "custom_palette = [\"#457b9d\", \"#ef233c\"]\n",
    "sns.scatterplot(data=df, x=\"age\", y=\"bmi\", hue=\"stroke\", palette=custom_palette, alpha=0.7)\n",
    "plt.title(\"Scatter Plot of Age vs BMI by Stroke Incidence\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"BMI\")\n",
    "plt.legend(title=\"Stroke\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplot for Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"stroke\", palette=\"husl\", corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Encoding \n",
    "- Convert Residence_type to binary\n",
    "- Convert work_type to separate binary columns \n",
    "- Convert smoking_status to binary columns\n",
    "- Drop the original 'Residence_type', 'work_type', and 'smoking_status' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Residence_type'] = df['Residence_type'].apply(lambda x: 1 if x == 'Urban' else 0)\n",
    "print(\"After converting Residence_type to binary (0 = Rural, 1 = Urban):\")\n",
    "display(df[['Residence_type']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_type_dummies = pd.get_dummies(df['work_type'], prefix='work_type')\n",
    "df = pd.concat([df, work_type_dummies[['work_type_Never_worked', 'work_type_Private', 'work_type_Self-employed']]], axis=1)\n",
    "\n",
    "# Display the first few rows to verify the transformation\n",
    "print(\"\\nAfter converting work_type to binary columns:\")\n",
    "print(df[['work_type_Never_worked', 'work_type_Private', 'work_type_Self-employed']].head())\n",
    "\n",
    "# Ensure all binary columns contain only 0 or 1\n",
    "work_type_columns = ['work_type_Never_worked', 'work_type_Private', 'work_type_Self-employed']\n",
    "for col in work_type_columns:\n",
    "    df[col] = df[col].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "print(\"\\nAfter ensuring binary columns contain only 0 or 1:\")\n",
    "print(df[work_type_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking_status_dummies = pd.get_dummies(df['smoking_status'], prefix='smoking_status')\n",
    "df = pd.concat([df, smoking_status_dummies[['smoking_status_never smoked', 'smoking_status_formerly smoked', 'smoking_status_smokes']]], axis=1)\n",
    "\n",
    "# Display the first few rows to verify the transformation\n",
    "print(\"\\nAfter converting smoking_status to binary columns:\")\n",
    "print(df[['smoking_status_never smoked', 'smoking_status_formerly smoked', 'smoking_status_smokes']].head())\n",
    "\n",
    "# Ensure that all binary columns contain only 0 or 1\n",
    "binary_columns = ['smoking_status_never smoked', 'smoking_status_formerly smoked', 'smoking_status_smokes']\n",
    "for col in binary_columns:\n",
    "    df[col] = df[col].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "print(\"\\nAfter ensuring binary columns contain only 0 or 1:\")\n",
    "print(df[binary_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.drop(columns=['Residence_type', 'work_type', 'smoking_status'])\n",
    "print(\"\\nFinal transformed dataset for data modeling:\")\n",
    "display(df_model.head())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization\n",
    "- Encoding all columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for column in df.columns:\n",
    "    # Ensure the column is a Series and has a dtype\n",
    "    if isinstance(df[column], pd.Series):\n",
    "        # Check if column has object type (usually represents strings or categorical data)\n",
    "        if df[column].dtype == 'object':\n",
    "            # If it has only two unique values, use binary encoding\n",
    "            if df[column].nunique() == 2:\n",
    "                # Map the values directly to 0 and 1\n",
    "                df[column] = df[column].map({df[column].unique()[0]: 1, df[column].unique()[1]: 0})\n",
    "            # If it has more than two unique values, use one-hot encoding\n",
    "            else:\n",
    "                df = pd.get_dummies(df, columns=[column], drop_first=True)\n",
    "        # If column is boolean (True/False), convert to 0/1\n",
    "        elif df[column].dtype == bool:\n",
    "            df[column] = df[column].astype(int)\n",
    "\n",
    "# Convert any boolean columns (not already converted) to integers (True/False to 1/0)\n",
    "df = df.applymap(lambda x: 1 if x is True else (0 if x is False else x))\n",
    "\n",
    "# Remove duplicate columns (in case of one-hot encoding)\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "# Check the transformed DataFrame\n",
    "print(\"\\nEncoded DataFrame:\")\n",
    "print(df.head())  # Display the first few rows of the encoded DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression Model\n",
    "- Accuracy\n",
    "- RSME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By importing Ridge Regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "X = df.drop(columns='stroke')  # Drop the 'stroke' column from the features\n",
    "y = df['stroke'].values  # Target variable\n",
    "\n",
    "# Handle missing values (if any)\n",
    "df = df.dropna()  # Drop rows with missing values (if any)\n",
    "X = df.drop(columns='stroke')  # Re-define X after dropping NaNs\n",
    "y = df['stroke'].values  # Re-define y after dropping NaNs\n",
    "\n",
    "# Step 2: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Step 3: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Define and train the Ridge Regression model\n",
    "ridge_modell = Ridge(alpha=0.5)  # Set alpha (regularization strength)\n",
    "ridge_modell.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_predd = ridge_modell.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "y_pred_classr = (y_predd >= 0.14).astype(int)  # Convert predictions to binary classes (0 or 1)\n",
    "accuracy = accuracy_score(y_test, y_pred_classr)\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_predd))\n",
    "print(\"Accuracy:\", accuracy*100)\n",
    "print(\"RMSE:\", rmse*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without importing Ridge Regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "class RidgeRegression:\n",
    "    def __init__(self, lr=0.01, n_iters=2000, alpha=0.5):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.alpha = alpha\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_pred - y)) + (self.alpha / num_samples) * self.weights\n",
    "            db = (1 / num_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "# Train and predict\n",
    "ridge_model = RidgeRegression(lr=0.01, n_iters=500, alpha=0.5)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Classification threshold and accuracy\n",
    "y_pred_class = (y_pred >= 0.14).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Display results\n",
    "print(\"Accuracy:\", accuracy * 100, \"%\")\n",
    "print(\"RMSE:\", rmse*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Model\n",
    "- RSME\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without importing linearregression \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.001, n_iters=2000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / num_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "\n",
    "# Assuming X_train_scaled, X_test_scaled, y_train, y_test are already defined\n",
    "linear_model = LinearRegression(lr=0.01, n_iters=500)\n",
    "linear_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = linear_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert continuous predictions to binary (if target is binary classification)\n",
    "y_pred_class = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Calculate Accuracy (only if the target is binary)\n",
    "if set(y_test).issubset({0, 1}):\n",
    "    accuracy = accuracy_score(y_test, y_pred_class)\n",
    "    print(\"Accuracy:\", accuracy * 100, \"%\")\n",
    "else:\n",
    "    print(\"Accuracy cannot be computed for non-binary target variables.\")\n",
    "\n",
    "# Display RMSE\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By importing Linear Regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error # directy importing LinearRegression()\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "linear_reg_predictions = linear_reg.predict(X_test)\n",
    "linear_reg_rmse = np.sqrt(mean_squared_error(y_test, linear_reg_predictions))\n",
    "linear_reg_rmse*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_linear = np.mean((linear_reg_predictions.round() == y_test))\n",
    "print(\"Linear Regression Accuracy :\", accuracy_linear*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression \n",
    "- rsme\n",
    "- accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = Lasso()\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "lasso_reg_predictions = lasso_reg.predict(X_test)\n",
    "lasso_reg_rmse = np.sqrt(mean_squared_error(y_test, lasso_reg_predictions))\n",
    "lasso_reg_rmse*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_lasso = np.mean((lasso_reg_predictions .round() == y_test))\n",
    "print(\"Lasso Regression Accuracy :\", accuracy_lasso*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "- RSME\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "logistic_reg = LogisticRegression()  \n",
    "logistic_reg.fit(X_train, y_train)\n",
    "logistic_reg_predictions = logistic_reg.predict(X_test)\n",
    "logistic_reg_rmse = np.sqrt(mean_squared_error(y_test, logistic_reg_predictions))\n",
    "print(\"RSME:\",logistic_reg_rmse*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logistic = np.mean((logistic_reg_predictions.round() == y_test))\n",
    "print(\"Logistic Regression Accuracy :\", accuracy_logistic*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing and comparing all models: RSME and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example pre-calculated RMSE and Accuracy for each model\n",
    "models = ['Logistic Regression', 'Linear Regression', 'Lasso Regression', 'Ridge Regression']\n",
    "\n",
    "# Example RMSE and Accuracy values (replace these with your actual calculated values)\n",
    "rmse_values = [19.671435554683786\n",
    ", 18.12776784844222\n",
    ",18.952674831947895\n",
    ",18.128190549583962]  # Replace with actual RMSE values\n",
    "accuracy_values = [96.13034623217924\n",
    ", 96.23217922606925, 96.23217922606925, 92.4643584521385]  # Replace with actual accuracy values\n",
    "\n",
    "# Create x-axis positions for each model\n",
    "x = np.arange(len(models))  # Position of the models on x-axis\n",
    "width = 0.35  # Width of the bars for grouped bar chart\n",
    "\n",
    "# Create a grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create bars for RMSE and Accuracy\n",
    "bar1 = ax.bar(x - width/2, rmse_values, width, label='RMSE', color='#ef233c')\n",
    "bar2 = ax.bar(x + width/2, accuracy_values, width, label='Accuracy', color='#457b9d')\n",
    "\n",
    "# Add text labels on the bars\n",
    "ax.bar_label(bar1, fmt='%.2f', padding=5)\n",
    "ax.bar_label(bar2, fmt='%.2f', padding=5)\n",
    "\n",
    "# Set the labels and title\n",
    "ax.set_xlabel('Models', fontsize=14)\n",
    "ax.set_ylabel('Percentage ', fontsize=14)\n",
    "ax.set_title('Comparison of Models: RMSE and Accuracy', fontsize=16)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, fontsize=12)\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, Recall, F1 Score, Accuracy Score, Confusion Matrix for Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_pred_classr)\n",
    "recall = recall_score(y_test, y_pred_classr)\n",
    "f1 = f1_score(y_test, y_pred_classr)\n",
    "accuracy = accuracy_score(y_test, y_pred_classr)\n",
    "\n",
    "# Print results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classr)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"No Stroke\", \"Stroke\"])\n",
    "disp.plot(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, Recall, F1 Score, Accuracy Score, Confusion Matrix for Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Convert continuous predictions from Lasso Regression into binary predictions\n",
    "y_pred_class = (lasso_reg_predictions >= 0.05).astype(int)  # Apply threshold to convert to binary\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_pred_class)\n",
    "recall = recall_score(y_test, y_pred_class)\n",
    "f1 = f1_score(y_test, y_pred_class)\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "# Print results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"No Stroke\", \"Stroke\"])\n",
    "disp.plot(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, Recall, F1 Score, Accuracy Score, Confusion Matrix for Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Convert continuous predictions into binary predictions\n",
    "y_pred_class = ( logistic_reg_predictions>= 0.2).astype(int)  # Convert continuous predictions to binary (0 or 1)\n",
    "\n",
    "# Calculate metrics\n",
    "y_pred_class=logistic_reg_predictions\n",
    "precision = precision_score(y_test, y_pred_class)\n",
    "recall = recall_score(y_test, y_pred_class)\n",
    "f1 = f1_score(y_test, y_pred_class)\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "# Print results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"No Stroke\", \"Stroke\"])\n",
    "disp.plot(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, Recall, F1 Score, Accuracy Score, Confusion Matrix for Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "y_pred_class\n",
    "# Convert the continuous predictions into binary predictions\n",
    "y_pred_class = (linear_reg_predictions >= 0.23).astype(int)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "precision = precision_score(y_test, y_pred_class)\n",
    "recall = recall_score(y_test, y_pred_class)\n",
    "f1 = f1_score(y_test, y_pred_class)\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision Recall Curve -Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Predict on the test set using the trained ridge model\n",
    "y_pred_prob = ridge_model.predict(X_test_scaled)  # Get continuous predictions\n",
    "\n",
    "# Calculate precision and recall for Ridge regression\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.', color='#457b9d', label='Ridge Regression')\n",
    "plt.title('Precision-Recall Curve - Ridge Regression', fontsize=16)\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check dataset biased or not and its visual representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=df['stroke'].value_counts(normalize=True)\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 's' refers to the target variable column\n",
    "s = df['stroke']  # Replace 'stroke' with the actual column name if needed\n",
    "\n",
    "# Plotting the percentage distribution of the target variable\n",
    "s.value_counts(normalize=True).plot(kind='bar', color='#457b9d')\n",
    "plt.title(\"Distribution of Classes in Target Variable (Percentage)\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.xticks(rotation=0)  # Keeps the class labels horizontal\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
